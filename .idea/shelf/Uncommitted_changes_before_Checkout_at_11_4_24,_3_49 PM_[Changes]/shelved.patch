Index: src/relaqs/save_results.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import datetime\nimport os\nimport csv\nfrom relaqs import RESULTS_DIR\nfrom typing import List, Dict\nimport json\nimport numpy as np\nfrom types import MappingProxyType\n\nl = frozenset([])\nFrozenSetType = type(l)\nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, MappingProxyType):\n            return obj.copy()\n        if isinstance(obj, FrozenSetType):\n            return list(obj)\n        else:\n            return obj.__dict__         \n        return super(NpEncoder, self).default(obj)\n\nclass SaveResults():\n    def __init__(self,\n                 env=None,\n                 alg=None,\n                 results:List[Dict]=None,\n                 save_path=None,\n                 save_base_path=None,\n                 target_gate_string=None\n                ):\n        self.env = env\n        self.alg = alg\n        self.target_gate_string = target_gate_string\n        if save_path is None:\n            self.save_path = self.get_new_directory(save_base_path)\n        else:\n            self.save_path = save_path\n    \n        # Create directory if it does not exist\n        if not os.path.isdir(self.save_path):\n            os.makedirs(self.save_path)\n        self.results = results\n\n    def get_new_directory(self, save_base_path=None):\n        if save_base_path is None:\n            save_base_path = RESULTS_DIR\n\n        path = save_base_path + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S/\")\n\n        if self.target_gate_string is not None:\n            path = path[:-1] + \"_\"  + self.target_gate_string + \"/\"\n\n        return path\n\n    def save_env_transitions(self):\n        second_row = self.env.transition_history[1]\n        fidelity_n_space = len(str(second_row[0]))\n        fidelity = 'Fidelity'\n        fidelity = fidelity + ' '*(fidelity_n_space - len(fidelity))\n\n        rewards_n_space = len(str(second_row[1]))\n        rewards = 'Rewards'\n        rewards = rewards + ' '*(rewards_n_space - len(rewards))\n\n        actions_n_space = len(str(second_row[2]))\n        actions = 'Actions'\n        actions =  actions + ' '*(actions_n_space - len(actions))\n        \n        flattened_u_n_space = len( ','.join(map(str, second_row[3])))\n        flattened_u = 'Flattened U'\n        flattened_u =  flattened_u + ' '*(flattened_u_n_space - len(flattened_u))\n    \n        episode_n_space = len(str(second_row[4]))\n        episode = 'Episode Id'\n        episode = episode + ' '*(episode_n_space - len(episode)) \n\n        column_headers = [fidelity, rewards, actions, flattened_u, episode] \n        with open(self.save_path + \"env_data.csv\", \"w\") as f:\n            writer = csv.writer(f)\n            writer.writerow(column_headers)\n          \n            # Iterate over each row in the transition history\n            for row in self.env.transition_history:\n                # Convert the 'flattened_u' list within the row to a string\n                # Assuming 'flattened_u' is the fourth item in the row\n                row[3] = ','.join(map(str, row[3]))\n                # Write the modified row to the CSV\n                writer.writerow(row)\n\n        with open(self.save_path + \"env_data.npy\", \"wb\") as f:\n            np.save(f, np.array(self.env.transition_history))\n\n    def save_train_results_data(self):\n        with open(self.save_path+'train_results_data.json', 'w') as f:\n            json.dump(self.results,f, cls=NpEncoder)\n\n    def save_config(self, config_dict):\n        config_path = self.save_path + \"config.txt\"\n        with open(config_path, \"w\") as file:\n            for key, value in config_dict.items():\n                file.write(f\"{key}: {value}\\n\")\n\n    def save_model(self):\n        save_model_path = self.save_path + \"model_checkpoints/\"\n        self.alg.save(save_model_path)\n\n    def save_results(self):\n        if self.env is not None:\n            self.save_env_transitions()\n        if self.alg is not None:\n            self.save_config(self.alg.get_config().to_dict())\n            self.save_model()\n        if self.results is not None:\n            self.save_train_results_data()\n        return self.save_path\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/relaqs/save_results.py b/src/relaqs/save_results.py
--- a/src/relaqs/save_results.py	(revision 842345a121c8732c9ae9c76afaab4a8cf55a9c96)
+++ b/src/relaqs/save_results.py	(date 1730752882579)
@@ -71,20 +71,20 @@
         actions_n_space = len(str(second_row[2]))
         actions = 'Actions'
         actions =  actions + ' '*(actions_n_space - len(actions))
-        
+
         flattened_u_n_space = len( ','.join(map(str, second_row[3])))
         flattened_u = 'Flattened U'
         flattened_u =  flattened_u + ' '*(flattened_u_n_space - len(flattened_u))
-    
+
         episode_n_space = len(str(second_row[4]))
         episode = 'Episode Id'
-        episode = episode + ' '*(episode_n_space - len(episode)) 
+        episode = episode + ' '*(episode_n_space - len(episode))
 
-        column_headers = [fidelity, rewards, actions, flattened_u, episode] 
+        column_headers = [fidelity, rewards, actions, flattened_u, episode]
         with open(self.save_path + "env_data.csv", "w") as f:
             writer = csv.writer(f)
             writer.writerow(column_headers)
-          
+
             # Iterate over each row in the transition history
             for row in self.env.transition_history:
                 # Convert the 'flattened_u' list within the row to a string
@@ -95,6 +95,10 @@
 
         with open(self.save_path + "env_data.npy", "wb") as f:
             np.save(f, np.array(self.env.transition_history))
+        # columns = ['Fidelity', 'Rewards', 'Actions', 'Operator', 'Episode Id']
+        # df = pd.DataFrame(self.env.transition_history, columns=columns)
+        # df.to_pickle(self.save_path + "env_data.pkl")  # easier to load than csv
+        # df.to_csv(self.save_path + "env_data.csv", index=False)  # backup in case pickle doesn't work
 
     def save_train_results_data(self):
         with open(self.save_path+'train_results_data.json', 'w') as f:
Index: scripts/inferencing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from relaqs.save_results import SaveResults\nfrom relaqs.plot_data import plot_data\nimport numpy as np\nfrom relaqs.api.utils import do_inferencing, run\nfrom relaqs.api.gates import H\n\nnoise_file = \"april/ibmq_belem_month_is_4.json\"\ninferencing_noise_file = \"april/ibmq_manila_month_is_4.json\"\nn_episodes_for_inferencing = 10\nsave = True\nplot = True\nfigure_title = \"Inferencing with model\"\nn_training_iterations = 1\n\n# -----------------------> Training model <------------------------\nalg = run(gate=H(), \n        n_training_iterations=n_training_iterations, \n        noise_file=noise_file\n    )\n\n# -----------------------> Inferencing <---------------------------\nenv, alg = do_inferencing(alg, n_episodes_for_inferencing,quantum_noise_file_path=inferencing_noise_file)\n\n# -------------------> Save Inferencing Results <---------------------------------------\nsr = SaveResults(env, alg)\nsave_dir = sr.save_results()\nprint(\"Results saved to:\", save_dir)\n\n# ---------------------> Plot Data <-------------------------------------------\nassert save is True, \"If plot=True, then save must also be set to True\"\n\nplot_data(save_dir, episode_length=alg._episode_history[0].episode_length, figure_title=figure_title)\nprint(\"Plots Created\")\n# --------------------------------------------------------------\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/inferencing.py b/scripts/inferencing.py
--- a/scripts/inferencing.py	(revision 842345a121c8732c9ae9c76afaab4a8cf55a9c96)
+++ b/scripts/inferencing.py	(date 1730750736468)
@@ -12,15 +12,22 @@
 figure_title = "Inferencing with model"
 n_training_iterations = 1
 
+print("Starting Training")
+
 # -----------------------> Training model <------------------------
 alg = run(gate=H(), 
         n_training_iterations=n_training_iterations, 
         noise_file=noise_file
     )
 
+print("Ending Training")
+
+print("Starting Inferencing")
 # -----------------------> Inferencing <---------------------------
 env, alg = do_inferencing(alg, n_episodes_for_inferencing,quantum_noise_file_path=inferencing_noise_file)
 
+print("Ending Inferencing")
+
 # -------------------> Save Inferencing Results <---------------------------------------
 sr = SaveResults(env, alg)
 save_dir = sr.save_results()
@@ -32,4 +39,3 @@
 plot_data(save_dir, episode_length=alg._episode_history[0].episode_length, figure_title=figure_title)
 print("Plots Created")
 # --------------------------------------------------------------
-
Index: scripts/learning_other_gates.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\" Learning new single qubit gates, rather than the default X gate. \"\"\"\n\nimport ray\nfrom ray.rllib.algorithms.ddpg import DDPGConfig\nfrom ray.tune.registry import register_env\nfrom relaqs.environments.gate_synth_env_rllib_Haar import GateSynthEnvRLlibHaarNoisy\nfrom relaqs.save_results import SaveResults\nfrom relaqs.plot_data import plot_data\nimport relaqs.api.gates as gates\nimport numpy as np\n\ndef env_creator(config):\n    return GateSynthEnvRLlibHaarNoisy(config)\n\ndef run(n_training_iterations=1, save=True, plot=True):\n    ray.init()\n    register_env(\"my_env\", env_creator)\n    \n    # ---------------------> Configure algorithm and Environment <-------------------------\n    alg_config = DDPGConfig()\n    alg_config.framework(\"torch\")\n\n    env_config = GateSynthEnvRLlibHaarNoisy.get_default_env_config()\n\n    #env_config[\"U_target\"] = Gate.H\n    target_gate = gates.RandomSU2()\n    env_config[\"U_target\"] = target_gate.get_matrix()\n\n    alg_config.environment(\"my_env\", env_config=env_config)\n    #alg_config.environment(GateSynthEnvRLlibHaarNoisy, env_config=GateSynthEnvRLlibHaarNoisy.get_default_env_config())\n\n    alg_config.rollouts(batch_mode=\"complete_episodes\")\n    alg_config.train_batch_size = GateSynthEnvRLlibHaarNoisy.get_default_env_config()[\"steps_per_Haar\"]\n\n    ### working 1-3 sets\n    alg_config.actor_lr = 4e-5\n    alg_config.critic_lr = 5e-4\n\n    alg_config.actor_hidden_activation = \"relu\"\n    alg_config.critic_hidden_activation = \"relu\"\n    alg_config.num_steps_sampled_before_learning_starts = 1000\n    alg_config.actor_hiddens = [30,30,30]\n    alg_config.exploration_config[\"scale_timesteps\"] = 10000\n\n    alg = alg_config.build()\n    # ---------------------------------------------------------------------\n\n    # ---------------------> Train Agent <-------------------------\n    for _ in range(n_training_iterations):\n        result = alg.train()\n    # -------------------------------------------------------------\n\n    # ---------------------> Save Results <-------------------------\n    if save is True:\n        env = alg.workers.local_worker().env\n        sr = SaveResults(env, alg, target_gate_string=str(target_gate))\n        save_dir = sr.save_results()\n        print(\"Results saved to:\", save_dir)\n    # --------------------------------------------------------------\n\n    # ---------------------> Plot Data <-------------------------\n    if plot is True:\n        assert save is True, \"If plot=True, then save must also be set to True\"\n        plot_data(save_dir, episode_length=alg._episode_history[0].episode_length, figure_title=\"Random Target Gate\")\n        print(\"Plots Created\")\n    # --------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    #n_training_iterations = 500\n    n_training_iterations = 1\n    save = True\n    plot = False\n    run(n_training_iterations, save, plot)\n    \n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/learning_other_gates.py b/scripts/learning_other_gates.py
--- a/scripts/learning_other_gates.py	(revision 842345a121c8732c9ae9c76afaab4a8cf55a9c96)
+++ b/scripts/learning_other_gates.py	(date 1730752001751)
@@ -3,7 +3,7 @@
 import ray
 from ray.rllib.algorithms.ddpg import DDPGConfig
 from ray.tune.registry import register_env
-from relaqs.environments.gate_synth_env_rllib_Haar import GateSynthEnvRLlibHaarNoisy
+from relaqs.environments.gate_synth_env_rllib_Haar import GateSynthEnvRLlibHaarNoisy, GateSynthEnvRLlibHaar
 from relaqs.save_results import SaveResults
 from relaqs.plot_data import plot_data
 import relaqs.api.gates as gates
Index: src/relaqs/environments/gate_synth_env_rllib_Haar.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import gymnasium as gym\nimport numpy as np\nimport scipy.linalg as la\nimport cmath\nimport random\nfrom qutip.superoperator import liouvillian, spre, spost\nfrom qutip import Qobj, tensor\nfrom qutip.operators import *\nfrom qutip import cnot, cphase\n#from relaqs.api.reward_functions import negative_matrix_difference_norm\n\nsig_p = np.array([[0, 1], [0, 0]])\nsig_m = np.array([[0, 0], [1, 0]])\nX = np.array([[0, 1], [1, 0]])\nZ = np.array([[1, 0], [0, -1]])\nI = np.array([[1, 0], [0, 1]])\nY = np.array([[0, 1j], [-1j, 0]])\n\n\n#two-qubit single qubit gates\nII = tensor(Qobj(I),Qobj(I)).data.toarray()\nX1 = tensor(Qobj(X),Qobj(I)).data.toarray()\nX2 = tensor(Qobj(I),Qobj(X)).data.toarray()\nY1 = tensor(Qobj(Y),Qobj(I)).data.toarray()\nY2 = tensor(Qobj(I),Qobj(Y)).data.toarray()\nZ1 = tensor(Qobj(Z),Qobj(I)).data.toarray()\nZ2 = tensor(Qobj(I),Qobj(Z)).data.toarray()\n\nsig_p1 = tensor(Qobj(sig_p),Qobj(I)).data.toarray()\nsig_p2 = tensor(Qobj(I),Qobj(sig_p)).data.toarray()\nsig_m1 = tensor(Qobj(sig_m),Qobj(I)).data.toarray()\nsig_m2 = tensor(Qobj(I),Qobj(sig_m)).data.toarray()\nsigmap1 = Qobj(sig_p1)\nsigmap2 = Qobj(sig_p2)\nsigmam1 = Qobj(sig_m1)\nsigmam2 = Qobj(sig_m2)\n\n#two-qubit gate basis\nXX = tensor(Qobj(X),Qobj(X)).data.toarray()\nYY = tensor(Qobj(Y),Qobj(Y)).data.toarray()\nZZ = tensor(Qobj(Z),Qobj(Z)).data.toarray()\nexchangeOperator = tensor(Qobj(sig_p),Qobj(sig_m)).data.toarray() + tensor(Qobj(sig_m),Qobj(sig_p)).data.toarray()\n\nCNOT = cnot().data.toarray()\nCZ = cphase(np.pi).data.toarray()\n\nclass GateSynthEnvRLlibHaar(gym.Env):\n    @classmethod\n    def get_default_env_config(cls):\n        return {\n            \"action_space_size\": 3,\n            \"U_initial\": I,\n            \"U_target\": X,\n            \"final_time\": 35.5556E-9, # in seconds\n            \"num_Haar_basis\": 1,\n            \"steps_per_Haar\": 2,  # steps per Haar basis per episode\n            \"delta\": 0,\n            \"save_data_every_step\": 1,\n            \"verbose\": True,\n            \"observation_space_size\": 9,  # 1 (fidelity) + 8 (flattened unitary)\n        }\n    def __init__(self, env_config):\n        self.final_time = env_config[\"final_time\"]  # Final time for the gates\n        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(env_config[\"observation_space_size\"],))\n        self.action_space = gym.spaces.Box(low=np.array([-1, -1, -1]), high=np.array([1, 1, 1]))\n        self.delta = env_config[\"delta\"]  # detuning\n        self.U_target = env_config[\"U_target\"]\n        self.U_initial = env_config[\"U_initial\"] # future todo, can make random initial state\n        self.U = env_config[\"U_initial\"]\n        self.num_Haar_basis = env_config[\"num_Haar_basis\"]\n        self.steps_per_Haar = env_config[\"steps_per_Haar\"]\n        self.verbose = env_config[\"verbose\"]\n        self.current_Haar_num = 1\n        self.current_step_per_Haar = 1\n        self.H_array = []\n        self.H_tot = []\n        self.U_array = []\n        self.state = self.unitary_to_observation(self.U)\n        self.prev_fidelity = 0\n        self.gamma_phase_max = 1.1675 * np.pi\n        self.gamma_magnitude_max = 1.8 * np.pi / self.final_time / self.steps_per_Haar\n        self.gamma_detuning_max = 0.05E9      #detuning of the control pulse in Hz \n        self.transition_history = []\n        self.episode_id = None\n\n    def unitary_to_observation(self, U):\n        return (\n            np.array(\n                [(abs(x), (cmath.phase(x) / np.pi + 1) / 2) for x in U.flatten()], \n                dtype=np.float64,\n            )\n            .squeeze()\n            .reshape(-1)\n        )\n\n    def get_observation(self):\n        return np.append([self.compute_fidelity()], self.unitary_to_observation(self.U))\n    \n    def compute_fidelity(self):\n        return float(np.abs(np.trace(self.U_target.conjugate().transpose() @ self.U))) / (self.U.shape[0])\n\n    def hamiltonian(self, delta, alpha, gamma_magnitude, gamma_phase):\n        \"\"\"Alpha and gamma are complex. This function could be made a callable class attribute.\"\"\"\n        return (delta + alpha) * Z + gamma_magnitude * (np.cos(gamma_phase) * X + np.sin(gamma_phase) * Y)\n\n    def reset(self, *, seed=None, options=None):\n        self.U = self.U_initial\n        starting_observeration = self.get_observation()\n        self.state = self.get_observation()\n        self.current_Haar_num = 1\n        self.current_step_per_Haar = 1\n        self.H_array = []\n        self.H_tot = []\n        self.U_array = []\n        self.prev_fidelity = 0\n        info = {}\n        self.episode_id = None\n        return starting_observeration, info\n\n    def step(self, action):\n        num_time_bins = 2 ** (self.current_Haar_num - 1) # Haar number decides the number of time bins\n\n        # gamma is the complex amplitude of the control field\n        gamma_magnitude = self.gamma_magnitude_max / 2 * (action[0] + 1)\n        gamma_phase = self.gamma_phase_max * action[1]\n        alpha = self.gamma_detuning_max * action[2]\n\n        H = self.hamiltonian(self.delta, alpha, gamma_magnitude, gamma_phase)\n        self.H_array.append(H)\n\n        self.H_tot = []\n\n        for ii, H_elem in enumerate(self.H_array):\n            for jj in range(0, num_time_bins):\n                Haar_num = self.current_Haar_num - np.floor(ii / self.steps_per_Haar) # Haar_num: label which Haar wavelet, current_Haar_num: order in the array\n                factor = (-1) ** np.floor(jj / (2 ** (Haar_num - 1)))\n                if ii > 0:\n                    self.H_tot[jj] += factor * H_elem\n                else:\n                    self.H_tot.append(factor * H_elem)\n\n        self.U = self.U_initial\n\n        for jj in range(0, num_time_bins):\n            Ut = la.expm(-1j * self.final_time / num_time_bins * self.H_tot[jj])\n            self.U = Ut @ self.U\n\n        self.U_array.append(self.U)\n\n        # Get reward (fidelity)\n        fidelity = self.compute_fidelity()\n        reward = (-3 * np.log10(1.0 - fidelity) + np.log10(1.0 - self.prev_fidelity)) + (3 * fidelity - self.prev_fidelity)\n        self.prev_fidelity = fidelity\n\n        self.state = self.get_observation()\n\n        # printing on the command line for quick viewing\n        if self.verbose is True:\n            print(\n                \"Step: \", f\"{self.current_step_per_Haar}\",\n                \"F: \", f\"{fidelity:7.3f}\",\n                \"R: \", f\"{reward:7.3f}\",\n                \"amp: \" f\"{action[0]:7.3f}\",\n                \"phase: \" f\"{action[1]:7.3f}\",\n                \"detuning: \" f\"{action[2]:7.3f}\"\n            )\n\n        self.transition_history.append([fidelity, reward, action, self.U.flatten(), self.episode_id])\n        \n        # Determine if episode is over\n        truncated = False\n        terminated = False\n        if fidelity >= 1:\n            truncated = True  # truncated when target fidelity reached\n        elif (self.current_Haar_num >= self.num_Haar_basis) and (self.current_step_per_Haar >= self.steps_per_Haar):  # terminate when all Haar is tested\n            terminated = True\n        else:\n            terminated = False\n\n        if (self.current_step_per_Haar == self.steps_per_Haar):  # For each Haar basis, if all trial steps ends, them move to next haar wavelet\n            self.current_Haar_num += 1\n            self.current_step_per_Haar = 1\n        else:\n            self.current_step_per_Haar += 1\n\n        info = {}\n        return (self.state, reward, terminated, truncated, info)\n\n\nclass GateSynthEnvRLlibHaarNoisy(gym.Env):\n    @classmethod\n    def get_default_env_config(cls):\n        return {\n            # \"action_space_size\": 3,\n            \"action_space_size\": 2,\n            \"U_initial\": I,  # staring with I\n            \"U_target\": X,  # target for X\n            \"final_time\": 35.5556E-9, # in seconds\n            \"num_Haar_basis\": 1,  # number of Haar basis (need to update for odd combinations)\n            \"steps_per_Haar\": 2,  # steps per Haar basis per episode\n            \"delta\": [0],  # qubit detuning\n            \"save_data_every_step\": 1,\n            \"verbose\": True,\n#            \"relaxation_rates_list\": [[0.01,0.02],[0.05, 0.07]], # relaxation lists of list of floats to be sampled from when resetting environment.\n#            \"relaxation_ops\": [sigmam(),sigmaz()] #relaxation operator lists for T1 and T2, respectively\n            \"relaxation_rates_list\": [[314159]], # relaxation lists of list of floats to be sampled from when resetting environment. (10 usec)\n            \"relaxation_ops\": [sigmam()], #relaxation operator lists for T1 and T2, respectively\n#            \"observation_space_size\": 35, # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 2 for relaxation rate\n            \"observation_space_size\": 2*16 + 1 + 1 + 1 # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 1 for relaxation rate + 1 for detuning\n        }\n\n    def __init__(self, env_config):\n        self.final_time = env_config[\"final_time\"]  # Final time for the gates\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(env_config[\"observation_space_size\"],))  # propagation operator elements + fidelity\n        # self.action_space = gym.spaces.Box(low=np.array([-1, -1, -1]), high=np.array([1, 1, 1])) # for detuning included control\n        self.action_space = gym.spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]))\n#        self.delta = [env_config[\"delta\"]]  # detuning\n        self.delta = env_config[\"delta\"]  # detuning\n        self.detuning = 0\n        self.detuning_update()\n        self.U_target = self.unitary_to_superoperator(env_config[\"U_target\"])\n        self.U_initial = self.unitary_to_superoperator(env_config[\"U_initial\"])\n        self.num_Haar_basis = env_config[\"num_Haar_basis\"]\n        self.steps_per_Haar = env_config[\"steps_per_Haar\"]\n        self.verbose = env_config[\"verbose\"]\n        self.relaxation_rates_list = env_config[\"relaxation_rates_list\"]\n        self.relaxation_ops = env_config[\"relaxation_ops\"]\n        self.relaxation_rate = self.get_relaxation_rate()\n        self.current_Haar_num = 1  # starting with 1\n        self.current_step_per_Haar = 1\n        self.H_array = []  # saving all H's with Haar wavelet to be multiplied\n        self.H_tot = []  # Haar wavelet multipied H summed up for each time bin\n        self.L_array = []  # Liouvillian for each time bin\n        self.U_array = []  # propagation operators for each time bin\n        self.U = self.U_initial.copy()  # multiplied propagtion operators\n        self.state = self.unitary_to_observation(self.U_initial)  # starting observation space\n        self.prev_fidelity = 0  # previous step' fidelity for rewarding\n        self.gamma_phase_max = 1.1675 * np.pi\n        self.gamma_magnitude_max = 1.8 * np.pi / self.final_time / self.steps_per_Haar\n        self.transition_history = []\n        self.episode_id = None\n\n    def detuning_update(self):\n        # Random detuning selection\n        if len(self.delta)==1:\n            self.detuning = self.delta[0]\n        else:\n            self.detuning = random.sample(self.delta,k=1)[0]\n            print(\"detuning: \", f\"{self.detuning}\")\n        \n\n    def unitary_to_superoperator(self, U):\n        return (spre(Qobj(U)) * spost(Qobj(U))).data.toarray()\n\n    def get_relaxation_rate(self):\n        relaxation_size = len(self.relaxation_ops)      #get number of relaxation ops\n        \n        sampled_rate_list = []\n        for ii in range(relaxation_size):\n            sampled_rate_list.append(random.sample(self.relaxation_rates_list[ii],k=1)[0])\n\n        return sampled_rate_list\n            \n    def get_observation(self):\n        normalizedDetuning = [(self.detuning - min(self.delta)+1E-15)/(max(self.delta)-min(self.delta)+1E-15)]\n        return np.append([self.compute_fidelity()]+[x//6283185 for x in self.relaxation_rate]+normalizedDetuning, self.unitary_to_observation(self.U)) #6283185 assuming 500 nanosecond relaxation is max\n    \n    def compute_fidelity(self):\n        env_config = GateSynthEnvRLlibHaarNoisy.get_default_env_config()\n        U_target_dagger = self.unitary_to_superoperator(env_config[\"U_target\"].conjugate().transpose())\n        return float(np.abs(np.trace(U_target_dagger @ self.U))) / (self.U.shape[0])\n\n    def unitary_to_observation(self, U):\n        return (\n            np.array(\n                [(abs(x), (cmath.phase(x) / np.pi + 1) / 2) for x in U.flatten()], \n                dtype=np.float64,\n                )\n            .squeeze()\n            .reshape(-1)  # cmath phase gives -pi to pi\n        )\n\n    def hamiltonian(self, delta, alpha, gamma_magnitude, gamma_phase):\n        \"\"\"Alpha and gamma are complex. This function could be made a callable class attribute.\"\"\"\n        return (delta + alpha) * Z + gamma_magnitude * (np.cos(gamma_phase) * X + np.sin(gamma_phase) * Y)\n\n    def reset(self, *, seed=None, options=None):\n        self.U = self.U_initial.copy()\n        self.state = self.get_observation()\n        self.current_Haar_num = 1\n        self.current_step_per_Haar = 1\n        self.H_array = []\n        self.H_tot = []\n        self.L_array = []\n        self.U_array = []\n        self.prev_fidelity = 0\n        self.episode_id = None\n        self.relaxation_rate = self.get_relaxation_rate()\n        self.detuning = 0\n        self.detuning_update()\n        starting_observeration = self.get_observation()\n        info = {}\n        return starting_observeration, info\n\n    def step(self, action):\n        num_time_bins = 2 ** (self.current_Haar_num - 1) # Haar number decides the number of time bins\n\n        # action space setting\n        alpha = 0  # in current simulation we do not adjust the detuning\n\n        # gamma is the complex amplitude of the control field\n        gamma_magnitude = self.gamma_magnitude_max / 2 * (action[0] + 1)\n        gamma_phase = self.gamma_phase_max * action[1]\n\n        # Set noise opertors\n        jump_ops = []\n        for ii in range(len(self.relaxation_ops)):\n            jump_ops.append(np.sqrt(self.relaxation_rate[ii]) * self.relaxation_ops[ii])\n\n        # Hamiltonian with controls\n        H = self.hamiltonian(self.detuning, alpha, gamma_magnitude, gamma_phase)\n        self.H_array.append(H)  # Array of Hs at each Haar wavelet\n\n        # H_tot for adding Hs at each time bins\n        self.H_tot = []\n\n        for ii, H_elem in enumerate(self.H_array):\n            for jj in range(0, num_time_bins):\n                Haar_num = self.current_Haar_num - np.floor(ii / self.steps_per_Haar) # Haar_num: label which Haar wavelet, current_Haar_num: order in the array\n                factor = (-1) ** np.floor(jj / (2 ** (Haar_num - 1))) # factor flips the sign every 2^(Haar_num-1)\n                if ii > 0:\n                    self.H_tot[jj] += factor * H_elem\n                else:  # Because H_tot[jj] does not exist\n                    self.H_tot.append(factor * H_elem)\n\n        self.L = ([])  # at every step we calculate L again because minimal time bin changes\n        self.U = np.eye(4)  # identity\n\n        for jj in range(0, num_time_bins):\n            L = (liouvillian(Qobj(self.H_tot[jj]), jump_ops, data_only=False, chi=None)).data.toarray()  # Liouvillian calc\n            self.L_array.append(L)\n            Ut = la.expm(self.final_time / num_time_bins * L)  # time evolution (propagation operator)\n            self.U = Ut @ self.U  # calculate total propagation until the time we are at\n\n        # Reward and fidelity calculation\n        fidelity = self.compute_fidelity()\n        reward = (-3 * np.log10(1.0 - fidelity) + np.log10(1.0 - self.prev_fidelity)) + (3 * fidelity - self.prev_fidelity)\n        #reward = negative_matrix_difference_norm(self.U_target, self.U)\n        self.prev_fidelity = fidelity\n\n        self.state = self.get_observation()\n\n        # printing on the command line for quick viewing\n        if self.verbose is True:\n            print(\n                \"Step: \", f\"{self.current_step_per_Haar}\" + \" episode id :\" + f\"{self.episode_id}\",\n                \"Relaxation rates:\")\n            for rate in self.relaxation_rate:\n                print(f\"{rate:7.6f}\")\n            print(\n                \"F: \", f\"{fidelity:7.3f}\",\n                \"R: \", f\"{reward:7.3f}\",\n                \"amp: \" f\"{action[0]:7.3f}\",\n                \"phase: \" f\"{action[1]:7.3f}\",\n            )\n\n        self.transition_history.append([fidelity, reward, action.tolist(), self.U.flatten(), self.episode_id])\n\n        # Determine if episode is over\n        truncated = False\n        terminated = False\n        if fidelity >= 1:\n            truncated = True  # truncated when target fidelity reached\n        elif (self.current_Haar_num >= self.num_Haar_basis) and (self.current_step_per_Haar >= self.steps_per_Haar):  # terminate when all Haar is tested\n            terminated = True\n        else:\n            terminated = False\n\n        if (self.current_step_per_Haar == self.steps_per_Haar):  # For each Haar basis, if all trial steps ends, them move to next haar wavelet\n            self.current_Haar_num += 1\n            self.current_step_per_Haar = 1\n        else:\n            self.current_step_per_Haar += 1\n\n        info = {}\n        return (self.state, reward, terminated, truncated, info)\n\n\nclass TwoQubitGateSynth(gym.Env):\n    @classmethod\n    def get_default_env_config(cls):\n        return {\n            \"action_space_size\": 7,\n            \"U_initial\": II,  # staring with I\n            \"U_target\": CZ,  # target for CZ\n            \"final_time\": 30E-9, # in seconds\n            \"num_Haar_basis\": 4,  # number of Haar basis (need to update for odd combinations)\n            \"steps_per_Haar\": 2,  # steps per Haar basis per episode\n            \"delta\": [[0],[0]],  # qubit detuning\n            \"save_data_every_step\": 1,\n            \"verbose\": True,\n#            \"relaxation_rates_list\": [[1/60E-6/2/np.pi],[1/30E-6/2/np.pi],[1/66E-6/2/np.pi],[1/5E-6/2/np.pi]], # relaxation lists of list of floats to be sampled from when resetting environment.\n            \"relaxation_rates_list\": [[0],[0],[0],[0]], # for now\n            \"relaxation_ops\": [sigmam1,sigmam2,Qobj(Z1),Qobj(Z2)], #relaxation operator lists for T1 and T2, respectively\n#            \"observation_space_size\": 35, # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 2 for relaxation rate\n            \"observation_space_size\": 2*256 + 1 + 4 + 2 # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 4 for relaxation rate + 2 for detuning\n        }\n\n    #physics: https://journals.aps.org/prapplied/pdf/10.1103/PhysRevApplied.10.054062, eq(2)\n    #parameters: https://journals.aps.org/prx/pdf/10.1103/PhysRevX.11.021058\n    #30 ns duration, g1 = 72.5 MHz, g2 = 71.5 MHz, g12 = 5 MHz\n    #T1 = 60 us, 30 us\n    #T2* = 66 us, 5 us\n\n    def hamiltonian(self, delta1, delta2, alpha1, alpha2, twoQubitDetuning, gamma_magnitude1, gamma_phase1, gamma_magnitude2, gamma_phase2, g1 = 72.5E6, g2 = 71.5E6, g12 = 5E6):\n        selfEnergyTerms = (delta1 + alpha1) * Z1 + (delta2 + alpha2) * Z2\n        Qubit1ControlTerms = gamma_magnitude1 * (np.cos(gamma_phase1) * X1 + np.sin(gamma_phase1) * Y1)\n        Qubit2ControlTerms = gamma_magnitude2 * (np.cos(gamma_phase2) * X2 + np.sin(gamma_phase2) * Y2)\n\n        #omega1 = delta1+alpha1, omega2 = delta2+alpha2, omegaC = alphaC\n#        Delta1 = delta1+alpha1-alphaC\n#        Delta2 = delta2+alpha2-alphaC\n#        twoQubitDetuning = 1/((1/Delta1 + 1/Delta2)/2)\n        \n        g_eff = g1*g2/twoQubitDetuning + g12\n        interactionEnergy = g_eff*exchangeOperator\n\n        energyTotal = selfEnergyTerms + interactionEnergy + Qubit1ControlTerms + Qubit2ControlTerms\n\n\n#        print(\"coupling: \", f\"{g_eff:7.3f}\")\n#        print(\"Delta1: \", f\"{Delta1:7.3f}\")\n#        print(\"Delta2: \", f\"{Delta2:7.3f}\")\n#        print(\"twoQubitDetuning: \", f\"{twoQubitDetuning:7.3f}\")\n\n        return energyTotal\n\n    def __init__(self, env_config):\n        self.final_time = env_config[\"final_time\"]  # Final time for the gates\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(env_config[\"observation_space_size\"],))  # propagation operator elements + fidelity + relaxation + detuning\n        self.action_space = gym.spaces.Box(low=-1*np.ones(7), high=np.ones(7)) #alpha1, alpha2, alphaC, gamma_magnitude1, gamma_phase1, gamma_magnitude2, gamma_phase2\n        self.delta = env_config[\"delta\"]  # detuning\n        self.detuning = [0, 0]\n        self.detuning_update()\n        self.U_target = self.unitary_to_superoperator(env_config[\"U_target\"])\n        self.U_initial = self.unitary_to_superoperator(env_config[\"U_initial\"])\n        self.num_Haar_basis = env_config[\"num_Haar_basis\"]\n        self.steps_per_Haar = env_config[\"steps_per_Haar\"]\n        self.verbose = env_config[\"verbose\"]\n        self.relaxation_rates_list = env_config[\"relaxation_rates_list\"]\n        self.relaxation_ops = env_config[\"relaxation_ops\"]\n        self.relaxation_rate = self.get_relaxation_rate()\n        self.current_Haar_num = 1  # starting with 1\n        self.current_step_per_Haar = 1\n        self.H_array = []  # saving all H's with Haar wavelet to be multiplied\n        self.H_tot = []  # Haar wavelet multipied H summed up for each time bin\n        self.L_array = []  # Liouvillian for each time bin\n        self.U_array = []  # propagation operators for each time bin\n        self.U = self.U_initial.copy()  # multiplied propagtion operators\n        self.state = self.unitary_to_observation(self.U_initial)  # starting observation space\n        self.prev_fidelity = 0  # previous step' fidelity for rewarding\n        self.alpha_max = 4*np.pi/self.final_time\n        #self.alpha_max = 0\n        #self.alphaC_mod_max = 1.5E9  ## see https://journals.aps.org/prx/pdf/10.1103/PhysRevX.11.021058\n        #self.alphaC_mod_max = 0.005E9  ## see https://journals.aps.org/prx/pdf/10.1103/PhysRevX.11.021058\n        #self.alphaC0 = 1.0367E9 # coupler center frequency : 5.2GHz, qubit 1 center frequency: 4.16 GHz\n        #self.alphaC0 = 0.01E9 # coupler center frequency : 5.2GHz, qubit 1 center frequency: 4.16 GHz        \n        self.Delta0 = 100E6 \n        self.Delta_mod_max = 25E6 \n        self.gamma_phase_max = 1.1675 * np.pi\n        self.gamma_magnitude_max = 1.8 * np.pi / self.final_time / self.steps_per_Haar\n        self.transition_history = []\n\n    def detuning_update(self):\n        # Random detuning selection\n        if len(self.delta[0])==1:\n            detuning1 = self.delta[0][0]\n        else:\n            detuning1 = random.sample(self.delta[0],k=1)[0]\n            \n        # Random detuning selection\n        if len(self.delta[1])==1:\n            detuning2 = self.delta[0][0]\n        else:\n            detuning2 = random.sample(self.delta[1],k=1)[0]\n\n        self.detuning = [detuning1, detuning2]\n        \n        \n\n    def unitary_to_superoperator(self, U):\n        return (spre(Qobj(U)) * spost(Qobj(U))).data.toarray()\n\n    def get_relaxation_rate(self):\n        relaxation_size = len(self.relaxation_ops)      #get number of relaxation ops\n        \n        sampled_rate_list = []\n        for ii in range(relaxation_size):\n            sampled_rate_list.append(random.sample(self.relaxation_rates_list[ii],k=1)[0])\n\n        return sampled_rate_list\n            \n    def get_observation(self):\n        normalizedDetuning = [(self.detuning[0] - min(self.delta[0])+1E-15)/(max(self.delta[0])-min(self.delta[0])+1E-15), (self.detuning[1] - min(self.delta[1])+1E-15)/(max(self.delta[1])-min(self.delta[1])+1E-15)]\n        return np.append([self.compute_fidelity()]+[x//6283185 for x in self.relaxation_rate]+normalizedDetuning, self.unitary_to_observation(self.U)) #6283185 assuming 500 nanosecond relaxation is max\n    \n    def compute_fidelity(self):\n        env_config = TwoQubitGateSynth.get_default_env_config()\n        U_target_dagger = self.unitary_to_superoperator(env_config[\"U_target\"].conjugate().transpose())\n        F = float(np.abs(np.trace(U_target_dagger @ self.U))) / (self.U.shape[0])\n        return F\n\n    def unitary_to_observation(self, U):\n        return (\n            np.array(\n                [(abs(x), (cmath.phase(x) / 2 / np.pi + 1) / 2) for x in U.flatten()], \n                dtype=np.float64,\n                )\n            .squeeze()\n            .reshape(-1)  # cmath phase gives -2pi to 2pi (?)\n        )\n\n    def reset(self, *, seed=None, options=None):\n        self.U = self.U_initial.copy()\n        self.state = self.get_observation()\n        self.current_Haar_num = 1\n        self.current_step_per_Haar = 1\n        self.H_array = []\n        self.H_tot = []\n        self.L_array = []\n        self.U_array = []\n        self.prev_fidelity = 0\n        self.relaxation_rate = self.get_relaxation_rate()\n        self.detuning = 0\n        self.detuning_update()\n        starting_observeration = self.get_observation()\n        info = {}\n        return starting_observeration, info\n\n    def step(self, action):\n        num_time_bins = 2 ** (self.current_Haar_num - 1) # Haar number decides the number of time bins\n\n        ### action space setting\n        alpha1 = self.alpha_max * action[0] \n        alpha2 = self.alpha_max * action[1] \n        #alphaC = self.alphaC0 + self.alphaC_mod_max * action[2] \n        Delta = self.Delta0 + self.Delta_mod_max * action[2]\n\n        # gamma is the complex amplitude of the control field\n        gamma_magnitude1 = self.gamma_magnitude_max / 2 * (action[3] + 1)\n        gamma_magnitude2 = self.gamma_magnitude_max / 2 * (action[4] + 1)\n\n        gamma_phase1 = self.gamma_phase_max * action[5] \n        gamma_phase2 = self.gamma_phase_max * action[6]\n\n        # Set noise opertors\n        jump_ops = []\n        for ii in range(len(self.relaxation_ops)):\n            jump_ops.append(np.sqrt(self.relaxation_rate[ii]) * self.relaxation_ops[ii])\n\n        # Hamiltonian with controls\n        H = self.hamiltonian(self.delta[0][0], self.delta[1][0], alpha1, alpha2, Delta, gamma_magnitude1, gamma_phase1, gamma_magnitude2, gamma_phase2)\n        self.H_array.append(H)  # Array of Hs at each Haar wavelet\n\n        # H_tot for adding Hs at each time bins\n        self.H_tot = []\n\n        for ii, H_elem in enumerate(self.H_array):\n            for jj in range(0, num_time_bins):\n                Haar_num = self.current_Haar_num - np.floor(ii / self.steps_per_Haar) # Haar_num: label which Haar wavelet, current_Haar_num: order in the array\n                factor = (-1) ** np.floor(jj / (2 ** (Haar_num - 1))) # factor flips the sign every 2^(Haar_num-1)\n                if ii > 0:\n                    self.H_tot[jj] += factor * H_elem\n                else:  # Because H_tot[jj] does not exist\n                    self.H_tot.append(factor * H_elem)\n\n        self.L = ([])  # at every step we calculate L again because minimal time bin changes\n        self.U = np.eye(16)  # identity\n\n        for jj in range(0, num_time_bins):\n            L = (liouvillian(Qobj(self.H_tot[jj]), jump_ops, data_only=False, chi=None)).data.toarray()  # Liouvillian calc\n            self.L_array.append(L)\n            Ut = la.expm(self.final_time / num_time_bins * L)  # time evolution (propagation operator)\n            self.U = Ut @ self.U  # calculate total propagation until the time we are at\n\n        # Reward and fidelity calculation\n        fidelity = self.compute_fidelity()\n        reward = (-5 * np.log10(1.0 - fidelity) + np.log10(1.0 - self.prev_fidelity)) + (5 * fidelity - self.prev_fidelity)\n        self.prev_fidelity = fidelity\n\n        self.state = self.get_observation()\n\n\n        # if self.verbose is True:\n        #     print(\n        #         \"F: \", f\"{fidelity:7.3f}\",\n        #         \"R: \", f\"{reward:7.3f}\",\n        #     )\n\n        self.transition_history.append([fidelity, reward, *action, *self.U.flatten()])\n\n        # Determine if episode is over\n        truncated = False\n        terminated = False\n        if fidelity >= 1:\n            truncated = True  # truncated when target fidelity reached\n        elif (self.current_Haar_num >= self.num_Haar_basis) and (self.current_step_per_Haar >= self.steps_per_Haar):  # terminate when all Haar is tested\n            terminated = True\n        else:\n            terminated = False\n\n        if (self.current_step_per_Haar == self.steps_per_Haar):  # For each Haar basis, if all trial steps ends, them move to next haar wavelet\n            self.current_Haar_num += 1\n            self.current_step_per_Haar = 1\n        else:\n            self.current_step_per_Haar += 1\n\n        info = {}\n        return (self.state, reward, terminated, truncated, info)\n\n\n
===================================================================
diff --git a/src/relaqs/environments/gate_synth_env_rllib_Haar.py b/src/relaqs/environments/gate_synth_env_rllib_Haar.py
--- a/src/relaqs/environments/gate_synth_env_rllib_Haar.py	(revision 842345a121c8732c9ae9c76afaab4a8cf55a9c96)
+++ b/src/relaqs/environments/gate_synth_env_rllib_Haar.py	(date 1730753354639)
@@ -206,7 +206,8 @@
             "relaxation_rates_list": [[314159]], # relaxation lists of list of floats to be sampled from when resetting environment. (10 usec)
             "relaxation_ops": [sigmam()], #relaxation operator lists for T1 and T2, respectively
 #            "observation_space_size": 35, # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 2 for relaxation rate
-            "observation_space_size": 2*16 + 1 + 1 + 1 # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 1 for relaxation rate + 1 for detuning
+#             "observation_space_size": 2*16 + 1 + 1 + 1 # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 1 for relaxation rate + 1 for detuning
+            "observation_space_size": 35 # 2*16 = (complex number)*(density matrix elements = 4)^2, + 1 for fidelity + 1 for relaxation rate + 1 for detuning
         }
 
     def __init__(self, env_config):
